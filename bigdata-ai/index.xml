<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>大数据与 AI on 腾讯云容器服务</title><link>https://book.kubetencent.io/bigdata-ai/</link><description>Recent content in 大数据与 AI on 腾讯云容器服务</description><generator>Hugo -- gohugo.io</generator><language>zh</language><atom:link href="https://book.kubetencent.io/bigdata-ai/index.xml" rel="self" type="application/rss+xml"/><item><title>GPU 虚拟化</title><link>https://book.kubetencent.io/bigdata-ai/gpu-share/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/gpu-share/</guid><description> GPU-Manager: https://docs.qq.com/slide/DU0JvQ2hwdEVicWl2</description></item><item><title>Flink on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/flink-on-tke-eks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink-on-tke-eks/</guid><description>2.2 job cluster 模式 Job cluster 模式，给每一个独立的Job 部署一整套Flink 集群；我们会给每一个job 制作一个容器镜像，并给它分配专用的资源，所以这个Job不用和其他Job 来通信，可以独立的扩缩容。
下面将详细解析如何通过job cluster 模式在kubernetes 上运行flink 任务， 主要有下面几个步骤： Compile and package the Flink job jar. Build a Docker image containing the Flink runtime and the job jar. Create a Kubernetes Job for Flink JobManager. Create a Kubernetes Service for this Job. Create a Kubernetes Deployment for Flink TaskManagers. Enable Flink JobManager HA with ZooKeeper. Correctly stop and resume Flink job with SavePoint facility.</description></item><item><title>Flink on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/flink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink/</guid><description>Flink On kubernetes 方案 将 Flink 部署到 Kubernetes 有以下几种集群部署方案:
Session Cluster: 相当于将静态部署的 Standalone Cluster 容器化，TaskManager 与 JobManager 都以 Deployment 方式部署，可动态提交 Job，Job 处理能力主要取决于 TaskManager 的配置 (slot/cpu/memory) 与副本数 (replicas)，调整副本数可以动态扩容。这种方式也是比较常见和成熟的方式。 Job Cluster: 相当于给每一个独立的 Job 部署一整套 Flink 集群，这套集群就只能运行一个 Job，配备专门制作的 Job 镜像，不能动态提交其它 Job。这种模式可以让每种 Job 拥有专用的资源，独立扩容。 Native Kubernetes: 这种方式是与 Kubernetes 原生集成，相比前面两种，这种模式能做到动态向 Kubernetes 申请资源，不需要提前指定 TaskManager 数量，就像 flink 与 yarn 和 mesos 集成一样。此模式能够提高资源利用率，但还处于试验阶段，不够成熟，不建议部署到生产环境。 参考官方文档: https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html#flink-session-cluster-on-kubernetes
Session Cluster 方式部署 准备资源文件(flink.yaml):
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.</description></item><item><title>Spark on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/spark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/spark/</guid><description>概念说明 Spark Application：由客户编写，并提交到spark框架中执行的用户代码逻辑。 Spark Driver：运行Spark Application代码逻辑中的main函数。 Spark Context：启动spark application的时候创建，作为Spark 运行时环境。 Spark Executor：负责执行单独的Task。 Spark Client mode：client模式下，driver可以运行在集群外。 Spark Cluster mode：cluster模式下，driver需要运行在集群中。 架构模式 spark on k8s最先只支持standalone模式，而自spark 2.3.0版本开始，支持kubernetes原生调度。
1 standalone mode
2 k8s native mode 处理流程如下：
spark-submit提交spark程序到k8s集群中。 spark-submit创建driver pod。 driver 创建executor pod，并把自己设置成executor pod的ownerReferences。 executor成功运行，并给driver上报ready 状态。 driver下发任务到executor 中执行。 当任务完成之后，executor 自动终止，并被driver 清理。 driver pod输出log，后维持在completed状态。 用户指南 编译 备注: 安装jdk1.8
# 下载 $ git clone https://github.com/apache/spark.git # 解压 $ mkdir -p /usr/local/bin $ tar -zxvf jdk-8u241-linux-x64.</description></item><item><title>TensorFlow on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/tensorflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/tensorflow/</guid><description>简介 Kubeflow 开始是从支持tensorflow分布式训练演化而来，现在已经是一个由众多子项目组成的开源产品，愿景是在 Kubernetes 上运行各种机器学习工作负载， 包括tensorflow,pytorch等主流计算引擎，面向机器学习场景的工作流引擎argo等。支持从模型开发，模型训练到最终服务部署的全链条工具。采用tf-operator进行分布式tf训练可以方便提交任务，具有重试，容错等优势
安装部署 安装 kubeflow curl -sL https://github.com/kubeflow/kfctl/releases/download/v1.0/kfctl_v1.0-0-g94c35cf_linux.tar.gz | tar -xzvf -C /home/ubuntu/kubeflow export PATH=$PATH:/home/ubuntu/kubeflow export KF_NAME=my-kubeflow export BASE_DIR=/home/ubuntu/kubeflow export KF_DIR=${BASE_DIR}/${KF_NAME} export CONFIG_URI=https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.0.yaml kfctl build -V -f $(CONFIG_URI) 安装 tf-operator kustomize build my-kubeflow/kustomize/tf-job-crds/overlays/application | kubectl create –f - kustomize build my-kubeflow/kustomize/tf-job-operator/overlays/application | kubectl create –f 测试 git clone https://github.com/tensorflow/k8s.git kubectl create –f k8s/examples/v1/dist-mnist/tf_job_mnist.yaml 使用kube-batch调度器 k8s缺省的调度器以pod为单位进行调度,也不提供按训练任务优先级优先级进行整体调度的功能，但是分布式机器学习会同时启动多种task，比如PS,WORKER，而且每种task都会有多个副本，这样有可能出现一个任务的部分pod被创建，而其他pod处于pending状态，出现资源被占用但是训练任务不能被启动执行的状态。kube-batch就是解决这种问题的批调度器，另外也提供队列和资源公平调度drf等功能。
部署kube-batch git clone http://github.com/kubernetes-sigs/kube-batch cd kube-batch/deployment/kube-batch helm install --name kube-batch --namespace kube-system 缺省安装后kube-batch缺乏必要的权限来lw集群资源，需要额外创建RBAC，添加对应serviceaccount的cluster-admin权限用于lw集群中的node, pod等对象来维护自身的资源视图</description></item></channel></rss>