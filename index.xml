<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>腾讯云容器服务指南 on 腾讯云容器服务</title><link>https://book.kubetencent.io/</link><description>Recent content in 腾讯云容器服务指南 on 腾讯云容器服务</description><generator>Hugo -- gohugo.io</generator><language>zh</language><atom:link href="https://book.kubetencent.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Nginx Ingress on TKE</title><link>https://book.kubetencent.io/ingress/other/nginx-ingress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/ingress/other/nginx-ingress/</guid><description/></item><item><title>彻底理解 Ingress</title><link>https://book.kubetencent.io/ingress/understand-ingress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/ingress/understand-ingress/</guid><description/></item><item><title>Ingress 方案选型</title><link>https://book.kubetencent.io/ingress/choose-ingress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/ingress/choose-ingress/</guid><description/></item><item><title>Traefik Ingress on TKE</title><link>https://book.kubetencent.io/ingress/other/traefik-ingress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/ingress/other/traefik-ingress/</guid><description/></item><item><title>TKE Ingress</title><link>https://book.kubetencent.io/ingress/tke-ingress/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/ingress/tke-ingress/</guid><description/></item><item><title>Flink 介绍</title><link>https://book.kubetencent.io/bigdata-ai/flink/intro/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink/intro/</guid><description>Flink 概述 Apache Flink 是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，它基于同一个Flink流式执行模型（streaming execution model），能够同时支持流处理和批处理两种应用类型。
由于流处理和批处理所提供的SLA(服务等级协议)是完全不相同，流处理一般需要支持低延迟、Exactly-once保证，而批处理需要支持高吞吐、高效处理。所以在实现的时候通常是分别给出两套实现方法，或者通过一个独立的开源框架来实现其中每一种处理方案； 比如：实现批处理的开源方案有MapReduce、Spark，实现流处理的开源方案有Storm，Spark的Streaming 其实本质上也是微批处理。
Flink在实现流处理和批处理时，与传统的一些方案完全不同，它从另一个视角看待流处理和批处理，将二者统一起来：Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。
流式框架的演进 Storm 是流式处理框架的先锋，实时处理能做到低延迟，但很难实现高吞吐，也不能保证精确一致性(exactly-once)，即保证执行一次并且只能执行一次。
后基于批处理框架 Spark 推出 Spark Streaming，将批处理数据分割的足够小，也实现了流失处理，并且可以做到高吞吐，能实现 exactly-once，但难以做到低时延，因为分割的任务之间需要有间隔时间，无法做到真实时。
最后 Flink 诞生了，同时做到了低延迟、高吞吐、exactly-once，并且还支持丰富的时间类型和窗口计算。
Flink 基本架构 JobManager 与 TaskManager Flink 主要由两个部分组件构成：JobManager 和 TaskManager。如何理解这两个组件的作用？JobManager 负责资源申请和任务分发，TaskManager 负责任务的执行。跟 k8s 本身类比，JobManager 相当于 Master，TaskManager 相当于 Worker；跟 Spark 类比，JobManager 相当于 Driver，TaskManager 相当于 Executor。
JobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端获取提交的任务，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。JobManager 是集群中的Master节点，整个集群有且仅有一个active的JobManager，负责整个集群的任务管理和资源管理。JobManager和TaskManager之间通过Actor System 进行通信，获取任务的执行情况并通过Actor System 将应用的任务的执行情况发送到客户端。同时在任务的执行过程中，Flink JobManager 会触发Checkpoints 操作，每个TaskManager 节点接受的到checkpoints触发命令后，完成checkpoints操作，所有的checkpoint协调过程都是在Flink JobManager中完成。当任务完成后，JobManager会将任务执行信息返回到客户端，并释放掉TaskManager中的资源以供下一次任务使用。
TaskManager 相当于整个集群的slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请与管理。客户端通过将编写好的flink应用编译打包，提交到JobManager，然后JobManager会根据已经注册在jobmanger中TaskManager的资源情况，将任务分配到有资源的TaskManager节点，然后启动并运行任务。TaskManager从JobManager那接受需要部署的任务，然后使用slot资源启动task，建立数据接入网络连接，接受数据并处理。同时TaskManager之间的数据交互都是通过数据流的方式进行的。</description></item><item><title>GPU 虚拟化</title><link>https://book.kubetencent.io/bigdata-ai/gpu-share/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/gpu-share/</guid><description> GPU-Manager: https://docs.qq.com/slide/DU0JvQ2hwdEVicWl2</description></item><item><title>TKE</title><link>https://book.kubetencent.io/intro/tke/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/intro/tke/</guid><description/></item><item><title>在 TKE 上搭建 Prometheus 监控系统</title><link>https://book.kubetencent.io/monitoring/deploy-prometheus-on-tke/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/monitoring/deploy-prometheus-on-tke/</guid><description>如果集群不在大陆，可以直接 Prometheus-operator 官方 helm chart 包安装:
Helm 2 Helm 3 kubectl create ns monitoring helm upgrade --install monitoring stable/prometheus-operator -n monitoring # helm repo add stable https://kubernetes-charts.storage.googleapis.com kubectl create ns monitoring helm install monitoring stable/prometheus-operator -n monitoring $(function(){$("#tab_with_code").tabs();}); 如果集群在大陆以内，连不上 helm 的 stable 仓库，可以使用这里定制的 chart (一些国内拉取不到的镜像同步到了腾讯云):
Helm 2 Helm 3 kubectl create ns monitoring helm repo add tencent https://tencentcloudcontainerteam.github.io/charts helm upgrade --install monitoring tencent/prometheus-operator -n monitoring kubectl create ns monitoring helm repo add tencent https://tencentcloudcontainerteam.</description></item><item><title>彻底理解集群网络</title><link>https://book.kubetencent.io/network/understand-cluster-networking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/network/understand-cluster-networking/</guid><description>什么是集群网络 TODO
K8S 网络模型 TODO
如何实现 K8S 集群网络 TODO
公有云 K8S 服务是如何实现集群网络的 TODO
CNI 插件 TODO
开源网络方案 TODO
参考资料 Cluster Networking: https://kubernetes.io/docs/concepts/cluster-administration/networking/</description></item><item><title>获取源 IP</title><link>https://book.kubetencent.io/containerization/source-ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/containerization/source-ip/</guid><description/></item><item><title>EKS</title><link>https://book.kubetencent.io/intro/eks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/intro/eks/</guid><description/></item><item><title>Session Cluster 模式部署</title><link>https://book.kubetencent.io/bigdata-ai/flink/session-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink/session-cluster/</guid><description>参考官方文档: https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html#flink-session-cluster-on-kubernetes
准备资源文件(flink.yaml):
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 1 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 jobmanager.heap.size: 1024m taskmanager.memory.process.size: 1024m log4j.properties: |+ log4j.rootLogger=INFO, file log4j.logger.akka=INFO log4j.logger.org.apache.kafka=INFO log4j.logger.org.apache.hadoop=INFO log4j.logger.org.apache.zookeeper=INFO log4j.appender.file=org.apache.log4j.FileAppender log4j.appender.file.file=${log.file} log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n log4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file --- apiVersion: apps/v1 kind: Deployment metadata: name: flink-jobmanager spec: replicas: 1 selector: matchLabels: app: flink component: jobmanager template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: flink:latest workingDir: /opt/flink command: [&amp;#34;/bin/bash&amp;#34;, &amp;#34;-c&amp;#34;, &amp;#34;$FLINK_HOME/bin/jobmanager.</description></item><item><title>TKE 集群网络介绍</title><link>https://book.kubetencent.io/network/tke-networking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/network/tke-networking/</guid><description>Global Router 方案 TODO</description></item><item><title>固定 IP</title><link>https://book.kubetencent.io/containerization/fixed-ip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/containerization/fixed-ip/</guid><description/></item><item><title>在 TKE 中安装 metrics-server</title><link>https://book.kubetencent.io/andon/install-metrics-server-on-tke/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/andon/install-metrics-server-on-tke/</guid><description> 下载 metrics-server repo: git clone https://github.com/kubernetes-incubator/metrics-server.git 修改 metrics-server 启动参数：--kubelet-insecure-tls ，防止 metrics server 访问 kubelet 采集指标时报证书问题(x509: certificate signed by unknown authority)， 在 deploy/1.8+/metrics-server-deployment.yaml 中加 args: containers: - name: metrics-server image: ccr.ccs.tencentyun.com/mirrors/metrics-server-amd64:v0.3.1 args: [&amp;#34;--kubelet-insecure-tls&amp;#34;] # 这里是新增的一行 imagePullPolicy: Always 在项目根目录执行: kubectl apply -f deploy/1.8+/ 注意是 apply 不是 create，apply 可以替换 kube-system 下的 apiservice，让 metric api 指向这个 metrics-server 4. 等待一小段时间(确保 metrics-server 采集到了 node 和 pod 的 metrics 指标数据)，通过下面的命令检验一下:
kubectl top pod --all-namespaces kubectl top node</description></item><item><title>Job Cluster 模式部署</title><link>https://book.kubetencent.io/bigdata-ai/flink/job-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink/job-cluster/</guid><description/></item><item><title>Spring Cloud</title><link>https://book.kubetencent.io/containerization/srpingcloud/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/containerization/srpingcloud/</guid><description/></item><item><title>TKE Mesh</title><link>https://book.kubetencent.io/intro/tke-mesh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/intro/tke-mesh/</guid><description/></item><item><title>网络划分与最大节点/service/pod 的数量</title><link>https://book.kubetencent.io/network/analysis-cidr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/network/analysis-cidr/</guid><description>创建集群时指定 CIDR 为集群网段，然后在选择 Pod数量上限/节点 和 Service数量上限/集群，最后会自动算出集群最大的节点数，这其中有计算公式。
集群网段会指定给 controller-manager 的 --cluster-cidr 参数。 选择 Service数量上限/集群 后会自动算出 service 网段，它是集群网段中的一个子网段，会将其指定給 controller-manager 的 --service-cluster-ip-range 参数。 选择 Pod数量上限/节点，即确定 PodCIDR 的掩码大小，会将其指定给 controller-manager 的 --node-cidr-mask-size 参数。 举一个例子：
/usr/bin/kube-controller-manager --cluster-cidr=10.99.0.0/19 --service-cluster-ip-range=10.99.28.0/22 --node-cidr-mask-size=24 --cluster-cidr=10.99.0.0/19 表示集群网络的 CIDR --service-cluster-ip-range=10.99.28.0/22 表示 Service 占用的子网(在TKE中是属于集群网络CIDR范围内的一个子网) TKE 默认每个节点的 CIDR 是 24 位，可以通过 kubectl describe node 查看 PodCIDR 字段来看，这里假设实际就是 24 位 此例中集群 Service 数量为：2^(32-22)=1024 个。公式：2 ^ (32 - SERVICE_CIDR_MASK_SIZE) 此例中集群节点最大数量：2^(24-19) - 2^(24-22) = 32 - 4 = 28 个 (Service占用4个节点子网段) 公式：2 ^ (POD_CIDR_MASK_SIZE - CLUSTER_CIDR_MASK_SIZE) - 2 ^ (POD_CIDR_MASK_SIZE - SERVICE_CIDR_MASK_SIZE) 此例中每个节点可以容纳 2^(32-24)=256 个 IP，减去网络地址、广播地址和子网为1的网桥 IP 地址(cbr0)，每个节点最多可以容纳 253 个 pod。但是节点 pod 实际最大容量还需要看 kubelet 启动参数 --max-pods 的值。通过 kubectl describe node 也能看到节点最大 pod 数 (Capacity.</description></item><item><title>Dubbo</title><link>https://book.kubetencent.io/containerization/dubbo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/containerization/dubbo/</guid><description/></item><item><title>Native Kubernetes 模式部署</title><link>https://book.kubetencent.io/bigdata-ai/flink/native-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink/native-kubernetes/</guid><description>与 Kubernetes 集成 在 flink 1.10 之前，在 k8s 上运行 flink 任务都是需要事先指定 TaskManager 的个数以及CPU和内存的，存在一个问题：大多数情况下，你在任务启动前根本无法精确的预估这个任务需要多少个TaskManager，如果指定多了，会导致资源浪费，指定少了，会导致任务调度不起来。本质原因是在 Kubernetes 上运行的 Flink 任务并没有直接向 Kubernetes 集群去申请资源。
在 2020-02-11 发布了 flink 1.10，该版本完成了与 k8s 集成的第一阶段，实现了向 k8s 动态申请资源，就像跟 yarn 或 mesos 集成那样。
部署步骤 确定 flink 部署的 namespace，这里我选 &amp;ldquo;flink&amp;rdquo;，确保 namespace 已创建:
kubectl create ns flink 创建 RBAC (创建 ServiceAccount 绑定 flink 需要的对 k8s 集群操作的权限):
apiVersion: v1 kind: ServiceAccount metadata: name: flink namespace: flink --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: flink-role-binding roleRef: apiGroup: rbac.</description></item><item><title>TCR</title><link>https://book.kubetencent.io/intro/tcr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/intro/tcr/</guid><description/></item><item><title>容器路由互通</title><link>https://book.kubetencent.io/network/container-route/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/network/container-route/</guid><description>概述 TKE 的 POD IP 默认在整个 VPC 可路由，要在 VPC 之外访问 POD IP 可能就需要下发路由(打通)才能访问，下面罗列容器路由互通的各种情况与容器路由打通方法。
云联网容器路由互通 如果需要容器路由互通的是两个不同 VPC，并且都支持云联网，那么可以直接将你的集群开启云联网，容器路由就可以自动注册上去，跨 VPC 的容器路由就可以互通了。
集群启用云联网方法：在集群信息页点击这里开启云联网
对等连接打通的 VPC 之间容器路由互通 如果两个 VPC 是同地域的，可以自行配置 VPC 路由表，在容器的对端 VPC (要访问容器 IP VPC) 的 VPC 路由表下一跳增加容器网段下一跳，指向对应的对等连接网关即可。 如果两个 VPC 是不同地域的，除了跟上面同地域容器互通操作之外，还需要提工单或找联系售后来做路由打通操作（实际就是下发容器路由给对等连接网关）。若有已创建的 TKE 集群，请提供集群id与对等连接id(pcx-开头)，否则提供：对等连接id、容器所在地域、vpcid (vpc-开头) 与容器网段。 专线打通的网络之间容器路由互通 如果您的 IDC 或办公网通过专线与腾讯云 VPC 打通，并且想要跟腾讯云 VPC 里的 TKE 容器路由互通，需要提工单或找联系售后来做路由打通操作（实际就是下发容器路由给专线网关）。
若有已创建的 TKE 集群，请提供集群id与专线网关id(pcg-开头)，否则提供对等连接id、容器所在地域、vpcid (vpc-开头) 、容器网段与专线网关id。如果您的 IDC 或办公网的网络是 BGP 的，可以自动学习容器路由，待容器路由打通之后，即可正常访问容器 IP，如果不是 BGP 则需要配置路由，容器网段下一跳到专线网关。
注意 路由打通之后，可以修改下集群中 ip-masq-agent 的配置来优化下:
kubectl -n kube-system edit configmap ip-masq-agent-config nonMasqueradeCIDRs 加上容器对端网络的网段，这样可以让从容器里主动请求对端网络的 IP 不做 SNAT，因为路由已经通了，出 VPC 的包就可以不用 SNAT，减少性能损耗。</description></item><item><title>更新历史</title><link>https://book.kubetencent.io/intro/changelog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/intro/changelog/</guid><description/></item><item><title>EKS 常见问题</title><link>https://book.kubetencent.io/faq/eks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/faq/eks/</guid><description>EKS 支持 hostPath 吗 不支持。serverless，没有 node，也就没有 hostPath
EKS 支持 kubectl exec -it 来登录容器吗 在灰度，需要开白名单，请提工单或联系售后开白。
EKS 如何让所有 pod 时区保持一致 由于不支持 hostPath，所以不能用此方法来挂载时区文件，可以通过挂载 configmap 来实现。
先通过时区文件创建 configmap:
kubectl create cm timezone-configmap --from-file=/usr/share/zoneinfo/Asia/Shanghai 再在 pod 里挂载 configmap:
apiVersion: v1 kind: Pod metadata: name: tz-configmap namespace: default annotations: eks.tke.cloud.tencent.com/cpu: &amp;#34;1&amp;#34; eks.tke.cloud.tencent.com/mem: 2Gi spec: restartPolicy: OnFailure containers: - name: busy-box-test env: - name: TZ value: Asia/Shanghai image: busybox imagePullPolicy: IfNotPresent command: [&amp;#34;sleep&amp;#34;, &amp;#34;60000&amp;#34;] volumeMounts: - name: timezone mountPath: /etc/localtimeX subPath: Shanghai volumes: - configMap: name: timezone-configmap items: - key: Shanghai path: Shanghai name: timezone 注: 由于 configmap 是 namespace 隔离的，如果要所有 pod 都时区同步，需要在所有 namespace 都创建时区的 configmap</description></item><item><title>Flink on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/flink-on-tke-eks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink-on-tke-eks/</guid><description>2.2 job cluster 模式 Job cluster 模式，给每一个独立的Job 部署一整套Flink 集群；我们会给每一个job 制作一个容器镜像，并给它分配专用的资源，所以这个Job不用和其他Job 来通信，可以独立的扩缩容。
下面将详细解析如何通过job cluster 模式在kubernetes 上运行flink 任务， 主要有下面几个步骤： Compile and package the Flink job jar. Build a Docker image containing the Flink runtime and the job jar. Create a Kubernetes Job for Flink JobManager. Create a Kubernetes Service for this Job. Create a Kubernetes Deployment for Flink TaskManagers. Enable Flink JobManager HA with ZooKeeper. Correctly stop and resume Flink job with SavePoint facility.</description></item><item><title>Flink on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/flink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/flink/</guid><description>Flink On kubernetes 方案 将 Flink 部署到 Kubernetes 有以下几种集群部署方案:
Session Cluster: 相当于将静态部署的 Standalone Cluster 容器化，TaskManager 与 JobManager 都以 Deployment 方式部署，可动态提交 Job，Job 处理能力主要取决于 TaskManager 的配置 (slot/cpu/memory) 与副本数 (replicas)，调整副本数可以动态扩容。这种方式也是比较常见和成熟的方式。 Job Cluster: 相当于给每一个独立的 Job 部署一整套 Flink 集群，这套集群就只能运行一个 Job，配备专门制作的 Job 镜像，不能动态提交其它 Job。这种模式可以让每种 Job 拥有专用的资源，独立扩容。 Native Kubernetes: 这种方式是与 Kubernetes 原生集成，相比前面两种，这种模式能做到动态向 Kubernetes 申请资源，不需要提前指定 TaskManager 数量，就像 flink 与 yarn 和 mesos 集成一样。此模式能够提高资源利用率，但还处于试验阶段，不够成熟，不建议部署到生产环境。 参考官方文档: https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html#flink-session-cluster-on-kubernetes
Session Cluster 方式部署 准备资源文件(flink.yaml):
apiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.</description></item><item><title>Spark on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/spark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/spark/</guid><description>概念说明 Spark Application：由客户编写，并提交到spark框架中执行的用户代码逻辑。 Spark Driver：运行Spark Application代码逻辑中的main函数。 Spark Context：启动spark application的时候创建，作为Spark 运行时环境。 Spark Executor：负责执行单独的Task。 Spark Client mode：client模式下，driver可以运行在集群外。 Spark Cluster mode：cluster模式下，driver需要运行在集群中。 架构模式 spark on k8s最先只支持standalone模式，而自spark 2.3.0版本开始，支持kubernetes原生调度。
1 standalone mode
2 k8s native mode 处理流程如下：
spark-submit提交spark程序到k8s集群中。 spark-submit创建driver pod。 driver 创建executor pod，并把自己设置成executor pod的ownerReferences。 executor成功运行，并给driver上报ready 状态。 driver下发任务到executor 中执行。 当任务完成之后，executor 自动终止，并被driver 清理。 driver pod输出log，后维持在completed状态。 用户指南 编译 备注: 安装jdk1.8
# 下载 $ git clone https://github.com/apache/spark.git # 解压 $ mkdir -p /usr/local/bin $ tar -zxvf jdk-8u241-linux-x64.</description></item><item><title>TensorFlow on TKE/EKS</title><link>https://book.kubetencent.io/bigdata-ai/tensorflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/bigdata-ai/tensorflow/</guid><description>简介 Kubeflow 开始是从支持tensorflow分布式训练演化而来，现在已经是一个由众多子项目组成的开源产品，愿景是在 Kubernetes 上运行各种机器学习工作负载， 包括tensorflow,pytorch等主流计算引擎，面向机器学习场景的工作流引擎argo等。支持从模型开发，模型训练到最终服务部署的全链条工具。采用tf-operator进行分布式tf训练可以方便提交任务，具有重试，容错等优势
安装部署 安装 kubeflow curl -sL https://github.com/kubeflow/kfctl/releases/download/v1.0/kfctl_v1.0-0-g94c35cf_linux.tar.gz | tar -xzvf -C /home/ubuntu/kubeflow export PATH=$PATH:/home/ubuntu/kubeflow export KF_NAME=my-kubeflow export BASE_DIR=/home/ubuntu/kubeflow export KF_DIR=${BASE_DIR}/${KF_NAME} export CONFIG_URI=https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.0.yaml kfctl build -V -f $(CONFIG_URI) 安装 tf-operator kustomize build my-kubeflow/kustomize/tf-job-crds/overlays/application | kubectl create –f - kustomize build my-kubeflow/kustomize/tf-job-operator/overlays/application | kubectl create –f 测试 git clone https://github.com/tensorflow/k8s.git kubectl create –f k8s/examples/v1/dist-mnist/tf_job_mnist.yaml 使用kube-batch调度器 k8s缺省的调度器以pod为单位进行调度,也不提供按训练任务优先级优先级进行整体调度的功能，但是分布式机器学习会同时启动多种task，比如PS,WORKER，而且每种task都会有多个副本，这样有可能出现一个任务的部分pod被创建，而其他pod处于pending状态，出现资源被占用但是训练任务不能被启动执行的状态。kube-batch就是解决这种问题的批调度器，另外也提供队列和资源公平调度drf等功能。
部署kube-batch git clone http://github.com/kubernetes-sigs/kube-batch cd kube-batch/deployment/kube-batch helm install --name kube-batch --namespace kube-system 缺省安装后kube-batch缺乏必要的权限来lw集群资源，需要额外创建RBAC，添加对应serviceaccount的cluster-admin权限用于lw集群中的node, pod等对象来维护自身的资源视图</description></item><item><title>TKE 常见问题</title><link>https://book.kubetencent.io/faq/tke/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/faq/tke/</guid><description>为什么挂载的 CFS 容量是 10G 创建负载的时候可以挂载 NFS，如果使用 CFS 提供的 NFS 服务，当 NFS 被挂载好去查看实际容量时发现是 10G。
原因是: nfs的盘，默认的挂载大小为10G，支持自动扩容
cfs 扩容方式: 小于1T，容量到50%触发自动扩容，每次加100G; 大于 1T,到80%触发自动扩容。每次加500G
cfs 收费方式: 计费是根据实际的使用量，小于10G不收费，多于10G开始收费
为什么 controller-manager 和 scheduler 状态显示 Unhealthy kubectl get cs NAME STATUS MESSAGE ERROR scheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused etcd-0 Healthy {&amp;#34;health&amp;#34;: &amp;#34;true&amp;#34;} 查看组件状态发现 controller-manager 和 scheduler 状态显示 Unhealthy，但是集群正常工作，是因为TKE metacluster托管方式集群的 apiserver 与 controller-manager 和 scheduler 不在同一个节点导致的，这个不影响功能。如果发现是 Healthy 说明 apiserver 跟它们部署在同一个节点，所以这个取决于部署方式。</description></item><item><title>审计管理</title><link>https://book.kubetencent.io/security/audit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/security/audit/</guid><description/></item><item><title>应用权限管理</title><link>https://book.kubetencent.io/security/app/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/security/app/</guid><description/></item><item><title>用户权限管理</title><link>https://book.kubetencent.io/security/user/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/security/user/</guid><description/></item><item><title>镜像仓库常见问题</title><link>https://book.kubetencent.io/faq/registry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://book.kubetencent.io/faq/registry/</guid><description>可以将自建的 harbor 镜像仓库同步到同步到 ccr 吗 当前 harbor 1.9 及以上可以直接在管理页面配置同步到 ccr，选择 docker registry 类型(ccr 兼容 registry 协议)。</description></item></channel></rss>