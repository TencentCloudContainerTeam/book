[{"uri":"https://book.kubetencent.io/ingress/other/nginx-ingress/","title":"Nginx Ingress on TKE","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/ingress/understand-ingress/","title":"彻底理解 Ingress","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/ingress/choose-ingress/","title":"Ingress 方案选型","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/ingress/other/traefik-ingress/","title":"Traefik Ingress on TKE","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/ingress/tke-ingress/","title":"TKE Ingress","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/bigdata-ai/flink/intro/","title":"Flink 介绍","tags":[],"description":"","content":"Flink 概述 Apache Flink 是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，它基于同一个Flink流式执行模型（streaming execution model），能够同时支持流处理和批处理两种应用类型。\n由于流处理和批处理所提供的SLA(服务等级协议)是完全不相同，流处理一般需要支持低延迟、Exactly-once保证，而批处理需要支持高吞吐、高效处理。所以在实现的时候通常是分别给出两套实现方法，或者通过一个独立的开源框架来实现其中每一种处理方案； 比如：实现批处理的开源方案有MapReduce、Spark，实现流处理的开源方案有Storm，Spark的Streaming 其实本质上也是微批处理。\nFlink在实现流处理和批处理时，与传统的一些方案完全不同，它从另一个视角看待流处理和批处理，将二者统一起来：Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。\n流式框架的演进 Storm 是流式处理框架的先锋，实时处理能做到低延迟，但很难实现高吞吐，也不能保证精确一致性(exactly-once)，即保证执行一次并且只能执行一次。\n后基于批处理框架 Spark 推出 Spark Streaming，将批处理数据分割的足够小，也实现了流失处理，并且可以做到高吞吐，能实现 exactly-once，但难以做到低时延，因为分割的任务之间需要有间隔时间，无法做到真实时。\n最后 Flink 诞生了，同时做到了低延迟、高吞吐、exactly-once，并且还支持丰富的时间类型和窗口计算。\nFlink 基本架构 JobManager 与 TaskManager Flink 主要由两个部分组件构成：JobManager 和 TaskManager。如何理解这两个组件的作用？JobManager 负责资源申请和任务分发，TaskManager 负责任务的执行。跟 k8s 本身类比，JobManager 相当于 Master，TaskManager 相当于 Worker；跟 Spark 类比，JobManager 相当于 Driver，TaskManager 相当于 Executor。\nJobManager 负责整个 Flink 集群任务的调度以及资源的管理，从客户端获取提交的任务，然后根据集群中 TaskManager 上 TaskSlot 的使用情况，为提交的应用分配相应的 TaskSlot 资源并命令 TaskManager 启动从客户端中获取的应用。JobManager 是集群中的Master节点，整个集群有且仅有一个active的JobManager，负责整个集群的任务管理和资源管理。JobManager和TaskManager之间通过Actor System 进行通信，获取任务的执行情况并通过Actor System 将应用的任务的执行情况发送到客户端。同时在任务的执行过程中，Flink JobManager 会触发Checkpoints 操作，每个TaskManager 节点接受的到checkpoints触发命令后，完成checkpoints操作，所有的checkpoint协调过程都是在Flink JobManager中完成。当任务完成后，JobManager会将任务执行信息返回到客户端，并释放掉TaskManager中的资源以供下一次任务使用。\nTaskManager 相当于整个集群的slave 节点，负责具体的任务执行和对应任务在每个节点上的资源申请与管理。客户端通过将编写好的flink应用编译打包，提交到JobManager，然后JobManager会根据已经注册在jobmanger中TaskManager的资源情况，将任务分配到有资源的TaskManager节点，然后启动并运行任务。TaskManager从JobManager那接受需要部署的任务，然后使用slot资源启动task，建立数据接入网络连接，接受数据并处理。同时TaskManager之间的数据交互都是通过数据流的方式进行的。\n有界数据流和无界数据流 Flink用于处理有界和无界数据：\n 无界数据流：无界数据流有一个开始但是没有结束，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。对于无界数据流我们无法等待所有数据都到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取event，以便能够推断结果完整性。 有界数据流：有界数据流有明确定义的开始和结束，可以在执行任何计算之前通过获取所有数据来处理有界流，处理有界流不需要有序获取，因为可以始终对有界数据集进行排序，有界流的处理也称为批处理。  编程模型 https://ci.apache.org/projects/flink/flink-docs-release-1.10/concepts/programming-model.html\nFlink On kubernetes 方案 将 Flink 部署到 Kubernetes 有以下几种集群部署方案:\n Session Cluster: 相当于将静态部署的 Standalone Cluster 容器化，TaskManager 与 JobManager 都以 Deployment 方式部署，可动态提交 Job，Job 处理能力主要取决于 TaskManager 的配置 (slot/cpu/memory) 与副本数 (replicas)，调整副本数可以动态扩容。这种方式也是比较常见和成熟的方式。 Job Cluster: 相当于给每一个独立的 Job 部署一整套 Flink 集群，这套集群就只能运行一个 Job，配备专门制作的 Job 镜像，不能动态提交其它 Job。这种模式可以让每种 Job 拥有专用的资源，独立扩容。 Native Kubernetes: 这种方式是与 Kubernetes 原生集成，相比前面两种，这种模式能做到动态向 Kubernetes 申请资源，不需要提前指定 TaskManager 数量，就像 flink 与 yarn 和 mesos 集成一样。此模式能够提高资源利用率，但还处于试验阶段，不够成熟，不建议部署到生产环境。  "},{"uri":"https://book.kubetencent.io/bigdata-ai/gpu-share/","title":"GPU 虚拟化","tags":[],"description":"","content":" GPU-Manager: https://docs.qq.com/slide/DU0JvQ2hwdEVicWl2  "},{"uri":"https://book.kubetencent.io/intro/tke/","title":"TKE","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/intro/","title":"产品指引","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/monitoring/deploy-prometheus-on-tke/","title":"在 TKE 上搭建 Prometheus 监控系统","tags":[],"description":"","content":"如果集群不在大陆，可以直接 Prometheus-operator 官方 helm chart 包安装:\n Helm 2 Helm 3  kubectl create ns monitoring helm upgrade --install monitoring stable/prometheus-operator -n monitoring   # helm repo add stable https://kubernetes-charts.storage.googleapis.com kubectl create ns monitoring helm install monitoring stable/prometheus-operator -n monitoring    $(function(){$(\"#tab_with_code\").tabs();}); 如果集群在大陆以内，连不上 helm 的 stable 仓库，可以使用这里定制的 chart (一些国内拉取不到的镜像同步到了腾讯云):\n Helm 2 Helm 3  kubectl create ns monitoring helm repo add tencent https://tencentcloudcontainerteam.github.io/charts helm upgrade --install monitoring tencent/prometheus-operator -n monitoring   kubectl create ns monitoring helm repo add tencent https://tencentcloudcontainerteam.github.io/charts helm install monitoring tencent/prometheus-operator -n monitoring    $(function(){$(\"#tab_with_code_2\").tabs();}); 更多自定义参数请参考: https://hub.helm.sh/charts/stable/prometheus-operator\n"},{"uri":"https://book.kubetencent.io/network/understand-cluster-networking/","title":"彻底理解集群网络","tags":[],"description":"","content":"什么是集群网络 TODO\nK8S 网络模型 TODO\n如何实现 K8S 集群网络 TODO\n公有云 K8S 服务是如何实现集群网络的 TODO\nCNI 插件 TODO\n开源网络方案 TODO\n参考资料  Cluster Networking: https://kubernetes.io/docs/concepts/cluster-administration/networking/  "},{"uri":"https://book.kubetencent.io/containerization/source-ip/","title":"获取源 IP","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/network/","title":"集群网络","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/intro/eks/","title":"EKS","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/ingress/","title":"Ingress","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/bigdata-ai/flink/session-cluster/","title":"Session Cluster 模式部署","tags":[],"description":"","content":"参考官方文档: https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html#flink-session-cluster-on-kubernetes\n准备资源文件(flink.yaml):\napiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 1 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 jobmanager.heap.size: 1024m taskmanager.memory.process.size: 1024m log4j.properties: |+ log4j.rootLogger=INFO, file log4j.logger.akka=INFO log4j.logger.org.apache.kafka=INFO log4j.logger.org.apache.hadoop=INFO log4j.logger.org.apache.zookeeper=INFO log4j.appender.file=org.apache.log4j.FileAppender log4j.appender.file.file=${log.file} log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n log4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file --- apiVersion: apps/v1 kind: Deployment metadata: name: flink-jobmanager spec: replicas: 1 selector: matchLabels: app: flink component: jobmanager template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: flink:latest workingDir: /opt/flink command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;$FLINK_HOME/bin/jobmanager.sh start;\\ while :; do if [[ -f $(find log -name \u0026#39;*jobmanager*.log\u0026#39; -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log; fi; done\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob - containerPort: 8081 name: ui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j.properties path: log4j.properties --- apiVersion: apps/v1 kind: Deployment metadata: name: flink-taskmanager spec: replicas: 2 selector: matchLabels: app: flink component: taskmanager template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: flink:latest workingDir: /opt/flink command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;$FLINK_HOME/bin/taskmanager.sh start; \\ while :; do if [[ -f $(find log -name \u0026#39;*taskmanager*.log\u0026#39; -print -quit) ]]; then tail -f -n +1 log/*taskmanager*.log; fi; done\u0026#34;] ports: - containerPort: 6122 name: rpc livenessProbe: tcpSocket: port: 6122 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf/ securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j.properties path: log4j.properties --- apiVersion: v1 kind: Service metadata: name: flink-jobmanager spec: type: ClusterIP ports: - name: rpc port: 6123 - name: blob port: 6124 - name: ui port: 8081 selector: app: flink component: jobmanager --- apiVersion: v1 kind: Service metadata: name: flink-jobmanager-rest spec: type: NodePort ports: - name: rest port: 8081 targetPort: 8081 selector: app: flink component: jobmanager 安装：\nkubectl apply -f flink.yaml 如何访问 JobManager 的 UI ？在 TKE 或者 EKS 上，支持 LoadBalancer 类型的 Service，可以将 JobManager 的 UI 用 LB 暴露：\nkubectl patch service flink-jobmanager -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;LoadBalancer\u0026#34;}}\u0026#39; 卸载：\nkubectl delete -f flink.yaml  若要部署到不同命名空间，请提前创建好命名空间并在所有 kubectl 命令后加 -n  "},{"uri":"https://book.kubetencent.io/network/tke-networking/","title":"TKE 集群网络介绍","tags":[],"description":"","content":"Global Router 方案 TODO\n"},{"uri":"https://book.kubetencent.io/containerization/fixed-ip/","title":"固定 IP","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/storage/","title":"存储","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/andon/install-metrics-server-on-tke/","title":"在 TKE 中安装 metrics-server","tags":[],"description":"","content":" 下载 metrics-server repo:  git clone https://github.com/kubernetes-incubator/metrics-server.git 修改 metrics-server 启动参数：--kubelet-insecure-tls ，防止 metrics server 访问 kubelet 采集指标时报证书问题(x509: certificate signed by unknown authority)， 在 deploy/1.8+/metrics-server-deployment.yaml 中加 args:  containers: - name: metrics-server image: ccr.ccs.tencentyun.com/mirrors/metrics-server-amd64:v0.3.1 args: [\u0026#34;--kubelet-insecure-tls\u0026#34;] # 这里是新增的一行 imagePullPolicy: Always 在项目根目录执行:  kubectl apply -f deploy/1.8+/ 注意是 apply 不是 create，apply 可以替换 kube-system 下的 apiservice，让 metric api 指向这个 metrics-server 4. 等待一小段时间(确保 metrics-server 采集到了 node 和 pod 的 metrics 指标数据)，通过下面的命令检验一下:\nkubectl top pod --all-namespaces kubectl top node "},{"uri":"https://book.kubetencent.io/andon/","title":"附加组件","tags":[],"description":"","content":"本章介绍在 TKE 和 EKS 如何安装附加组件以增强功能\n"},{"uri":"https://book.kubetencent.io/bigdata-ai/flink/job-cluster/","title":"Job Cluster 模式部署","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/containerization/srpingcloud/","title":"Spring Cloud","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/intro/tke-mesh/","title":"TKE Mesh","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/monitoring/","title":"监控告警","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/network/analysis-cidr/","title":"网络划分与最大节点/service/pod 的数量","tags":[],"description":"","content":"创建集群时指定 CIDR 为集群网段，然后在选择 Pod数量上限/节点 和 Service数量上限/集群，最后会自动算出集群最大的节点数，这其中有计算公式。\n 集群网段会指定给 controller-manager 的 --cluster-cidr 参数。 选择 Service数量上限/集群 后会自动算出 service 网段，它是集群网段中的一个子网段，会将其指定給 controller-manager 的 --service-cluster-ip-range 参数。 选择 Pod数量上限/节点，即确定 PodCIDR 的掩码大小，会将其指定给 controller-manager 的 --node-cidr-mask-size 参数。  举一个例子：\n/usr/bin/kube-controller-manager --cluster-cidr=10.99.0.0/19 --service-cluster-ip-range=10.99.28.0/22 --node-cidr-mask-size=24  --cluster-cidr=10.99.0.0/19 表示集群网络的 CIDR --service-cluster-ip-range=10.99.28.0/22 表示 Service 占用的子网(在TKE中是属于集群网络CIDR范围内的一个子网) TKE 默认每个节点的 CIDR 是 24 位，可以通过 kubectl describe node 查看 PodCIDR 字段来看，这里假设实际就是 24 位 此例中集群 Service 数量为：2^(32-22)=1024 个。公式：2 ^ (32 - SERVICE_CIDR_MASK_SIZE) 此例中集群节点最大数量：2^(24-19) - 2^(24-22) = 32 - 4 = 28 个 (Service占用4个节点子网段) 公式：2 ^ (POD_CIDR_MASK_SIZE - CLUSTER_CIDR_MASK_SIZE) - 2 ^ (POD_CIDR_MASK_SIZE - SERVICE_CIDR_MASK_SIZE) 此例中每个节点可以容纳 2^(32-24)=256 个 IP，减去网络地址、广播地址和子网为1的网桥 IP 地址(cbr0)，每个节点最多可以容纳 253 个 pod。但是节点 pod 实际最大容量还需要看 kubelet 启动参数 --max-pods 的值。通过 kubectl describe node 也能看到节点最大 pod 数 (Capacity.pods)。节点最大 pod 数计算公式：2 ^ (32 - POD_CIDR_MASK_SIZE) - 3  "},{"uri":"https://book.kubetencent.io/containerization/dubbo/","title":"Dubbo","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/bigdata-ai/flink/native-kubernetes/","title":"Native Kubernetes 模式部署","tags":[],"description":"","content":"与 Kubernetes 集成 在 flink 1.10 之前，在 k8s 上运行 flink 任务都是需要事先指定 TaskManager 的个数以及CPU和内存的，存在一个问题：大多数情况下，你在任务启动前根本无法精确的预估这个任务需要多少个TaskManager，如果指定多了，会导致资源浪费，指定少了，会导致任务调度不起来。本质原因是在 Kubernetes 上运行的 Flink 任务并没有直接向 Kubernetes 集群去申请资源。\n在 2020-02-11 发布了 flink 1.10，该版本完成了与 k8s 集成的第一阶段，实现了向 k8s 动态申请资源，就像跟 yarn 或 mesos 集成那样。\n部署步骤 确定 flink 部署的 namespace，这里我选 \u0026ldquo;flink\u0026rdquo;，确保 namespace 已创建:\nkubectl create ns flink 创建 RBAC (创建 ServiceAccount 绑定 flink 需要的对 k8s 集群操作的权限):\napiVersion: v1 kind: ServiceAccount metadata: name: flink namespace: flink --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: flink-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: edit subjects: - kind: ServiceAccount name: flink namespace: flink 利用 job 运行启动 flink 的引导程序 (请求 k8s 创建 jobmanager 相关的资源: service, deployment, configmap):\napiVersion: batch/v1 kind: Job metadata: name: boot-flink namespace: flink spec: template: spec: serviceAccount: flink restartPolicy: OnFailure containers: - name: start image: flink:1.10 workingDir: /opt/flink command: [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;$FLINK_HOME/bin/kubernetes-session.sh \\ -Dkubernetes.cluster-id=roc \\ -Dkubernetes.jobmanager.service-account=flink \\ -Dtaskmanager.memory.process.size=1024m \\ -Dkubernetes.taskmanager.cpu=1 \\ -Dtaskmanager.numberOfTaskSlots=1 \\ -Dkubernetes.container.image=flink:1.10 \\ -Dkubernetes.namespace=flink\u0026#34;]  kubernetes.cluster-id: 指定 flink 集群的名称，后续自动创建的 k8s 资源会带上这个作为前缀或后缀 kubernetes.namespace: 指定 flink 相关的资源创建在哪个命名空间，这里我们用 flink 命名空间 kubernetes.jobmanager.service-account: 指定我们刚刚为 flink 创建的 ServiceAccount kubernetes.container.image: 指定 flink 需要用的镜像，这里我们部署的 1.10 版本，所以镜像用 flink:1.10  部署完成后，我们可以看到有刚刚运行完成的 job 的 pod 和被这个 job 拉起的 flink jobmanager 的 pod，前缀与配置 kubernetes.cluster-id 相同:\n$ kubectl -n flink get pod NAME READY STATUS RESTARTS AGE roc-cf9f6b5df-csk9z 1/1 Running 0 84m boot-flink-nc2qx 0/1 Completed 0 84m 还有 jobmanager 的 service:\n$ kubectl -n flink get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE roc ClusterIP 172.16.255.152 \u0026lt;none\u0026gt; 8081/TCP,6123/TCP,6124/TCP 88m roc-rest LoadBalancer 172.16.255.11 150.109.27.251 8081:31240/TCP 88m 访问 http://150.109.27.251:8081 即可进入此 flink 集群的 ui 界面。\n参考资料  Active Kubernetes integration phase 2 - Advanced Features: https://issues.apache.org/jira/browse/FLINK-14460 Apache Flink 1.10.0 Release Announcement: https://flink.apache.org/news/2020/02/11/release-1.10.0.html Native Kubernetes Setup Beta (flink与kubernetes集成的官方教程): https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/deployment/native_kubernetes.html  "},{"uri":"https://book.kubetencent.io/intro/tcr/","title":"TCR","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/network/container-route/","title":"容器路由互通","tags":[],"description":"","content":"概述 TKE 的 POD IP 默认在整个 VPC 可路由，要在 VPC 之外访问 POD IP 可能就需要下发路由(打通)才能访问，下面罗列容器路由互通的各种情况与容器路由打通方法。\n云联网容器路由互通 如果需要容器路由互通的是两个不同 VPC，并且都支持云联网，那么可以直接将你的集群开启云联网，容器路由就可以自动注册上去，跨 VPC 的容器路由就可以互通了。\n集群启用云联网方法：在集群信息页点击这里开启云联网\n对等连接打通的 VPC 之间容器路由互通  如果两个 VPC 是同地域的，可以自行配置 VPC 路由表，在容器的对端 VPC (要访问容器 IP VPC) 的 VPC 路由表下一跳增加容器网段下一跳，指向对应的对等连接网关即可。 如果两个 VPC 是不同地域的，除了跟上面同地域容器互通操作之外，还需要提工单或找联系售后来做路由打通操作（实际就是下发容器路由给对等连接网关）。若有已创建的 TKE 集群，请提供集群id与对等连接id(pcx-开头)，否则提供：对等连接id、容器所在地域、vpcid (vpc-开头) 与容器网段。  专线打通的网络之间容器路由互通 如果您的 IDC 或办公网通过专线与腾讯云 VPC 打通，并且想要跟腾讯云 VPC 里的 TKE 容器路由互通，需要提工单或找联系售后来做路由打通操作（实际就是下发容器路由给专线网关）。\n若有已创建的 TKE 集群，请提供集群id与专线网关id(pcg-开头)，否则提供对等连接id、容器所在地域、vpcid (vpc-开头) 、容器网段与专线网关id。如果您的 IDC 或办公网的网络是 BGP 的，可以自动学习容器路由，待容器路由打通之后，即可正常访问容器 IP，如果不是 BGP 则需要配置路由，容器网段下一跳到专线网关。\n注意 路由打通之后，可以修改下集群中 ip-masq-agent 的配置来优化下:\nkubectl -n kube-system edit configmap ip-masq-agent-config nonMasqueradeCIDRs 加上容器对端网络的网段，这样可以让从容器里主动请求对端网络的 IP 不做 SNAT，因为路由已经通了，出 VPC 的包就可以不用 SNAT，减少性能损耗。\n"},{"uri":"https://book.kubetencent.io/log/","title":"日志搜集","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/cicd/","title":"CI/CD","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/bigdata-ai/flink/","title":"Flink","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/bigdata-ai/","title":"大数据与 AI","tags":[],"description":"","content":"本章介绍在 TKE 和 EKS 上的大数据与 AI 场景下的解决方案与实践\n"},{"uri":"https://book.kubetencent.io/intro/changelog/","title":"更新历史","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/faq/","title":"FAQ","tags":[],"description":"","content":"介绍使用 TKE/EKS 的一些常见问题\n"},{"uri":"https://book.kubetencent.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/faq/eks/","title":"EKS 常见问题","tags":[],"description":"","content":"EKS 支持 hostPath 吗 不支持。serverless，没有 node，也就没有 hostPath\nEKS 支持 kubectl exec -it 来登录容器吗 在灰度，需要开白名单，请提工单或联系售后开白。\nEKS 如何让所有 pod 时区保持一致 由于不支持 hostPath，所以不能用此方法来挂载时区文件，可以通过挂载 configmap 来实现。\n先通过时区文件创建 configmap:\nkubectl create cm timezone-configmap --from-file=/usr/share/zoneinfo/Asia/Shanghai 再在 pod 里挂载 configmap:\napiVersion: v1 kind: Pod metadata: name: tz-configmap namespace: default annotations: eks.tke.cloud.tencent.com/cpu: \u0026#34;1\u0026#34; eks.tke.cloud.tencent.com/mem: 2Gi spec: restartPolicy: OnFailure containers: - name: busy-box-test env: - name: TZ value: Asia/Shanghai image: busybox imagePullPolicy: IfNotPresent command: [\u0026#34;sleep\u0026#34;, \u0026#34;60000\u0026#34;] volumeMounts: - name: timezone mountPath: /etc/localtimeX subPath: Shanghai volumes: - configMap: name: timezone-configmap items: - key: Shanghai path: Shanghai name: timezone  注: 由于 configmap 是 namespace 隔离的，如果要所有 pod 都时区同步，需要在所有 namespace 都创建时区的 configmap\n EKS 支持从自定义镜像仓库中拉去镜像么 目前还不支持。 EKS 现在只能从腾讯云官方镜像仓库CCR https://console.cloud.tencent.com/tke2/registry/user?rid=1 及 TCR https://console.cloud.tencent.com/tcr 中拉去镜像。可以考虑将自己的镜像仓库迁移到腾讯云CCR/TCR; Harbor 可以直接同步镜像到CCR/TCR 。\nEKS 如何收集容器日志 EKS日志功能主要通过下面的环境变量来控制，如不加下面环境变量，则不收集日志。\n name: EKS_LOGS_OUTPUT_TYPE 日志收集到哪里，目前支持kafka, cls（腾讯云日志服务） value: kafka name: EKS_LOGS_KAFKA_HOST kafka host； 如果多个host ，用分号隔开 value: 10.0.16.42 name: EKS_LOGS_KAFKA_PORT kafka port value: \u0026ldquo;9092\u0026rdquo; name: EKS_LOGS_KAFKA_TOPIC kafka topic value: eks name: EKS_LOGS_METADATA_ON 是否收集 eks 的metadata 信息 value: \u0026ldquo;true\u0026rdquo; name: EKS_LOGS_LOG_PATHS 支持收集标准输出和 文件路径； 如同时收集，用分开隔开； value: stdout;/tmp/busy*.log  参考下面demo\napiVersion: apps/v1beta2 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026quot;1\u0026quot; labels: k8s-app: kafka qcloud-app: kafka name: kafka namespace: default spec: replicas: 1 selector: matchLabels: k8s-app: kafka qcloud-app: kafka template: metadata: annotations: eks.tke.cloud.tencent.com/cpu: \u0026quot;0.25\u0026quot; eks.tke.cloud.tencent.com/mem: \u0026quot;0.5Gi\u0026quot; labels: k8s-app: kafka qcloud-app: kafka spec: containers: - env: - name: EKS_LOGS_OUTPUT_TYPE value: kafka - name: EKS_LOGS_KAFKA_HOST value: 10.0.16.42 - name: EKS_LOGS_KAFKA_PORT value: \u0026quot;9092\u0026quot; - name: EKS_LOGS_KAFKA_TOPIC value: eks - name: EKS_LOGS_METADATA_ON value: \u0026quot;false\u0026quot; - name: EKS_LOGS_LOG_PATHS value: stdout;/tmp/busy*.log image: busybox:latest command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;while true; do echo hello world; date; echo hello \u0026gt;\u0026gt; /tmp/busy.log; sleep 1; done\u0026quot;] imagePullPolicy: Always name: while resources: requests: cpu: 250m memory: 512Mi EKS 如何正确创建 service service 已支持以下类型的service\n ClusterIP 用于同一集群内的其他服务或容器访问，支持TCP/UDP协议，支持设置ClientIP: none（headless service） Loadbalancer 用于集群外的服务访问，支持TCP/UDP协议  注意: VPC内网访问 需要设置如下的annotation: service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxxxxx\n# loadbalancer apiVersion: v1 kind: Service metadata: annotations: service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxxxxx # vpc内网访问 name: test namespace: default spec: externalTrafficPolicy: Cluster ports: - name: tcp-80-80 nodePort: 31688 port: 80 protocol: TCP targetPort: 80 sessionAffinity: None type: LoadBalancer --- # ClusterIP service apiVersion: v1 kind: Service metadata: name: nginx namespace: default spec: ports: - name: tcp-80-80 port: 80 protocol: TCP targetPort: 80 selector: k8s-app: nginx qcloud-app: nginx sessionAffinity: None type: ClusterIP --- # headless service apiVersion: v1 kind: Service metadata: name: nginx-headless namespace: default spec: clusterIP: None ports: - name: tcp-80-80 port: 80 protocol: TCP targetPort: 80 selector: k8s-app: nginx qcloud-app: nginx sessionAffinity: None type: ClusterIP "},{"uri":"https://book.kubetencent.io/bigdata-ai/flink-on-tke-eks/","title":"Flink on TKE/EKS","tags":[],"description":"","content":"2.2 job cluster 模式 Job cluster 模式，给每一个独立的Job 部署一整套Flink 集群；我们会给每一个job 制作一个容器镜像，并给它分配专用的资源，所以这个Job不用和其他Job 来通信，可以独立的扩缩容。\n下面将详细解析如何通过job cluster 模式在kubernetes 上运行flink 任务， 主要有下面几个步骤： Compile and package the Flink job jar. Build a Docker image containing the Flink runtime and the job jar. Create a Kubernetes Job for Flink JobManager. Create a Kubernetes Service for this Job. Create a Kubernetes Deployment for Flink TaskManagers. Enable Flink JobManager HA with ZooKeeper. Correctly stop and resume Flink job with SavePoint facility.\n2.2.1 编写一个Flink 流式处理任务 我们创建一个简单的流式处理任务；这个任务是从网络上读取数据流，5秒钟统计一次单词个数并输出; 核心代码如下：\nDataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026raquo; dataStream = env .socketTextStream(\u0026ldquo;10.0.100.12\u0026rdquo;, 9999) .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1);\ndataStream.print();\n10.0.100.12 是产生数据流的机器地址；在启动jobcluster 之前，通过nc -lk 9999, 来启动服务。\n完整的maven 项目地址：https://github.com/jizhang/flink-on-kubernetes 编译打包 mvn clean package , 最终可以找到编译好的 jar 包 flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar\n2.2.2 制作镜像 基于官方 flink 基础镜像，https://hub.docker.com/_/flink 制作业务镜像: FROM ccr.ccs.tencentyun.com/caryguo/flink:latest\nCOPY \u0026ndash;chown=flink:flink flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar /opt/flink/lib/\nUSER flink\ndocker build -t ccr.ccs.tencentyun.com/caryguo/count-word-flink-on-kubernetes:0.0.1 .\n2.3.2 部署JobManager apiVersion: batch/v1 kind: Job metadata: name: flink-on-kubernetes-jobmanager spec: template: metadata: labels: app: flink instance: flink-on-kubernetes-jobmanager spec: restartPolicy: OnFailure containers: - name: jobmanager image: ccr.ccs.tencentyun.com/caryguo/count-word-flink-on-kubernetes:0.0.1 command: [\u0026quot;/opt/flink/bin/standalone-job.sh\u0026rdquo;] args: [\u0026ldquo;start-foreground\u0026rdquo;, \u0026ldquo;-Djobmanager.rpc.address=flink-on-kubernetes-jobmanagerr\u0026rdquo;, \u0026ldquo;-Dparallelism.default=1\u0026rdquo;, \u0026ldquo;-Dblob.server.port=6124\u0026rdquo;, \u0026ldquo;-Dqueryable-state.server.ports=6125\u0026rdquo;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob - containerPort: 6125 name: query - containerPort: 8081 name: ui\nhttp://shzhangji.com/blog/2019/08/24/deploy-flink-job-cluster-on-kubernetes/\n2.3 operator 模式 google flink kubernetes operator\n参考 http://shzhangji.com/blog/2019/08/24/deploy-flink-job-cluster-on-kubernetes/ 知乎案例\n"},{"uri":"https://book.kubetencent.io/bigdata-ai/flink/","title":"Flink on TKE/EKS","tags":[],"description":"","content":"Flink On kubernetes 方案 将 Flink 部署到 Kubernetes 有以下几种集群部署方案:\n Session Cluster: 相当于将静态部署的 Standalone Cluster 容器化，TaskManager 与 JobManager 都以 Deployment 方式部署，可动态提交 Job，Job 处理能力主要取决于 TaskManager 的配置 (slot/cpu/memory) 与副本数 (replicas)，调整副本数可以动态扩容。这种方式也是比较常见和成熟的方式。 Job Cluster: 相当于给每一个独立的 Job 部署一整套 Flink 集群，这套集群就只能运行一个 Job，配备专门制作的 Job 镜像，不能动态提交其它 Job。这种模式可以让每种 Job 拥有专用的资源，独立扩容。 Native Kubernetes: 这种方式是与 Kubernetes 原生集成，相比前面两种，这种模式能做到动态向 Kubernetes 申请资源，不需要提前指定 TaskManager 数量，就像 flink 与 yarn 和 mesos 集成一样。此模式能够提高资源利用率，但还处于试验阶段，不够成熟，不建议部署到生产环境。  参考官方文档: https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html#flink-session-cluster-on-kubernetes\nSession Cluster 方式部署 准备资源文件(flink.yaml):\napiVersion: v1 kind: ConfigMap metadata: name: flink-config labels: app: flink data: flink-conf.yaml: |+ jobmanager.rpc.address: flink-jobmanager taskmanager.numberOfTaskSlots: 1 blob.server.port: 6124 jobmanager.rpc.port: 6123 taskmanager.rpc.port: 6122 jobmanager.heap.size: 1024m taskmanager.memory.process.size: 1024m log4j.properties: |+ log4j.rootLogger=INFO, file log4j.logger.akka=INFO log4j.logger.org.apache.kafka=INFO log4j.logger.org.apache.hadoop=INFO log4j.logger.org.apache.zookeeper=INFO log4j.appender.file=org.apache.log4j.FileAppender log4j.appender.file.file=${log.file} log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n log4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file --- apiVersion: apps/v1 kind: Deployment metadata: name: flink-jobmanager spec: replicas: 1 selector: matchLabels: app: flink component: jobmanager template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: flink:latest workingDir: /opt/flink command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;$FLINK_HOME/bin/jobmanager.sh start;\\ while :; do if [[ -f $(find log -name \u0026#39;*jobmanager*.log\u0026#39; -print -quit) ]]; then tail -f -n +1 log/*jobmanager*.log; fi; done\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob - containerPort: 8081 name: ui livenessProbe: tcpSocket: port: 6123 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j.properties path: log4j.properties --- apiVersion: apps/v1 kind: Deployment metadata: name: flink-taskmanager spec: replicas: 2 selector: matchLabels: app: flink component: taskmanager template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: flink:latest workingDir: /opt/flink command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;$FLINK_HOME/bin/taskmanager.sh start; \\ while :; do if [[ -f $(find log -name \u0026#39;*taskmanager*.log\u0026#39; -print -quit) ]]; then tail -f -n +1 log/*taskmanager*.log; fi; done\u0026#34;] ports: - containerPort: 6122 name: rpc livenessProbe: tcpSocket: port: 6122 initialDelaySeconds: 30 periodSeconds: 60 volumeMounts: - name: flink-config-volume mountPath: /opt/flink/conf/ securityContext: runAsUser: 9999 # refers to user _flink_ from official flink image, change if necessary volumes: - name: flink-config-volume configMap: name: flink-config items: - key: flink-conf.yaml path: flink-conf.yaml - key: log4j.properties path: log4j.properties --- apiVersion: v1 kind: Service metadata: name: flink-jobmanager spec: type: ClusterIP ports: - name: rpc port: 6123 - name: blob port: 6124 - name: ui port: 8081 selector: app: flink component: jobmanager --- apiVersion: v1 kind: Service metadata: name: flink-jobmanager-rest spec: type: NodePort ports: - name: rest port: 8081 targetPort: 8081 selector: app: flink component: jobmanager 安装：\nkubectl apply -f flink.yaml 如何访问 JobManager 的 UI ？在 TKE 或者 EKS 上，支持 LoadBalancer 类型的 Service，可以将 JobManager 的 UI 用 LB 暴露：\nkubectl patch service flink-jobmanager -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;LoadBalancer\u0026#34;}}\u0026#39; 卸载：\nkubectl delete -f flink.yaml  若要部署到不同命名空间，请提前创建好命名空间并在所有 kubectl 命令后加 -n  Job Cluster 模式部署 Job cluster 模式，给每一个独立的Job 部署一整套Flink 集群；我们会给每一个job 制作一个容器镜像，并给它分配专用的资源，所以这个Job不用和其他Job 来通信，可以独立的扩缩容。\n下面将详细解析如何通过job cluster 模式在kubernetes 上运行flink 任务， 主要有下面几个步骤：\n Compile and package the Flink job jar. Build a Docker image containing the Flink runtime and the job jar. Create a Kubernetes Job for Flink JobManager. Create a Kubernetes Service for this Job. Create a Kubernetes Deployment for Flink TaskManagers. Enable Flink JobManager HA with ZooKeeper. Correctly stop and resume Flink job with SavePoint facility.  编写一个Flink 流式处理任务 我们创建一个简单的流式处理任务；这个任务是从网络上读取数据流，5秒钟统计一次单词个数并输出; 核心代码如下：\nDataStream\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; dataStream = env .socketTextStream(\u0026#34;10.0.100.12\u0026#34;, 9999) .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); dataStream.print(); 10.0.100.12 是产生数据流的机器地址；在启动jobcluster 之前，通过nc -lk 9999, 来启动服务。\n完整的maven 项目地址：https://github.com/jizhang/flink-on-kubernetes 编译打包 mvn clean package , 最终可以找到编译好的 jar 包 flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar\n制作镜像 基于官方 flink 基础镜像，https://hub.docker.com/_/flink 制作业务镜像:\nFROMccr.ccs.tencentyun.com/caryguo/flink:latestCOPY --chown=flink:flink flink-on-kubernetes-0.0.1-SNAPSHOT-jar-with-dependencies.jar /opt/flink/lib/USERflinkdocker build -t ccr.ccs.tencentyun.com/caryguo/count-word-flink-on-kubernetes:0.0.1 .部署 JobManager apiVersion: batch/v1 kind: Job metadata: name: flink-on-kubernetes-jobmanager spec: template: metadata: labels: app: flink instance: flink-on-kubernetes-jobmanager spec: restartPolicy: OnFailure containers: - name: jobmanager image: ccr.ccs.tencentyun.com/caryguo/count-word-flink-on-kubernetes:0.0.1 command: [\u0026#34;/opt/flink/bin/standalone-job.sh\u0026#34;] args: [\u0026#34;start-foreground\u0026#34;, \u0026#34;-Djobmanager.rpc.address=flink-on-kubernetes-jobmanagerr\u0026#34;, \u0026#34;-Dparallelism.default=1\u0026#34;, \u0026#34;-Dblob.server.port=6124\u0026#34;, \u0026#34;-Dqueryable-state.server.ports=6125\u0026#34;] ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob - containerPort: 6125 name: query - containerPort: 8081 name: ui 参考： http://shzhangji.com/blog/2019/08/24/deploy-flink-job-cluster-on-kubernetes/\nNative Kuubernetes 方式部署 在 flink 1.10 之前，在 k8s 上运行 flink 任务都是需要事先指定 TaskManager 的个数以及CPU和内存的，存在一个问题：大多数情况下，你在任务启动前根本无法精确的预估这个任务需要多少个TaskManager，如果指定多了，会导致资源浪费，指定少了，会导致任务调度不起来。本质原因是在 Kubernetes 上运行的 Flink 任务并没有直接向 Kubernetes 集群去申请资源。\n在 2020-02-11 发布了 flink 1.10，该版本完成了与 k8s 集成的第一阶段，实现了向 k8s 动态申请资源，就像跟 yarn 或 mesos 集成那样。\n部署步骤 确定 flink 部署的 namespace，这里我选 \u0026ldquo;flink\u0026rdquo;，确保 namespace 已创建:\nkubectl create ns flink 创建 RBAC (创建 ServiceAccount 绑定 flink 需要的对 k8s 集群操作的权限):\napiVersion: v1 kind: ServiceAccount metadata: name: flink namespace: flink --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: flink-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: edit subjects: - kind: ServiceAccount name: flink namespace: flink 利用 job 运行启动 flink 的引导程序 (请求 k8s 创建 jobmanager 相关的资源: service, deployment, configmap):\napiVersion: batch/v1 kind: Job metadata: name: boot-flink namespace: flink spec: template: spec: serviceAccount: flink restartPolicy: OnFailure containers: - name: start image: flink:1.10 workingDir: /opt/flink command: [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;$FLINK_HOME/bin/kubernetes-session.sh \\ -Dkubernetes.cluster-id=roc \\ -Dkubernetes.jobmanager.service-account=flink \\ -Dtaskmanager.memory.process.size=1024m \\ -Dkubernetes.taskmanager.cpu=1 \\ -Dtaskmanager.numberOfTaskSlots=1 \\ -Dkubernetes.container.image=flink:1.10 \\ -Dkubernetes.namespace=flink\u0026#34;]  kubernetes.cluster-id: 指定 flink 集群的名称，后续自动创建的 k8s 资源会带上这个作为前缀或后缀 kubernetes.namespace: 指定 flink 相关的资源创建在哪个命名空间，这里我们用 flink 命名空间 kubernetes.jobmanager.service-account: 指定我们刚刚为 flink 创建的 ServiceAccount kubernetes.container.image: 指定 flink 需要用的镜像，这里我们部署的 1.10 版本，所以镜像用 flink:1.10  部署完成后，我们可以看到有刚刚运行完成的 job 的 pod 和被这个 job 拉起的 flink jobmanager 的 pod，前缀与配置 kubernetes.cluster-id 相同:\n$ kubectl -n flink get pod NAME READY STATUS RESTARTS AGE roc-cf9f6b5df-csk9z 1/1 Running 0 84m boot-flink-nc2qx 0/1 Completed 0 84m 还有 jobmanager 的 service:\n$ kubectl -n flink get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE roc ClusterIP 172.16.255.152 \u0026lt;none\u0026gt; 8081/TCP,6123/TCP,6124/TCP 88m roc-rest LoadBalancer 172.16.255.11 150.109.27.251 8081:31240/TCP 88m 访问 http://150.109.27.251:8081 即可进入此 flink 集群的 ui 界面。\n参考资料  Active Kubernetes integration phase 2 - Advanced Features: https://issues.apache.org/jira/browse/FLINK-14460 Apache Flink 1.10.0 Release Announcement: https://flink.apache.org/news/2020/02/11/release-1.10.0.html Native Kubernetes Setup Beta (flink与kubernetes集成的官方教程): https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/deployment/native_kubernetes.html  operator 模式 google flink kubernetes operator\n参考: http://shzhangji.com/blog/2019/08/24/deploy-flink-job-cluster-on-kubernetes/\n"},{"uri":"https://book.kubetencent.io/serverless/","title":"Serverless","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/mesh/","title":"Servie Mesh","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/bigdata-ai/spark/","title":"Spark on TKE/EKS","tags":[],"description":"","content":"概念说明  Spark Application：由客户编写，并提交到spark框架中执行的用户代码逻辑。 Spark Driver：运行Spark Application代码逻辑中的main函数。 Spark Context：启动spark application的时候创建，作为Spark 运行时环境。 Spark Executor：负责执行单独的Task。 Spark Client mode：client模式下，driver可以运行在集群外。 Spark Cluster mode：cluster模式下，driver需要运行在集群中。  架构模式 spark on k8s最先只支持standalone模式，而自spark 2.3.0版本开始，支持kubernetes原生调度。\n1 standalone mode\n2 k8s native mode 处理流程如下：\n spark-submit提交spark程序到k8s集群中。 spark-submit创建driver pod。 driver 创建executor pod，并把自己设置成executor pod的ownerReferences。 executor成功运行，并给driver上报ready 状态。 driver下发任务到executor 中执行。 当任务完成之后，executor 自动终止，并被driver 清理。 driver pod输出log，后维持在completed状态。  用户指南 编译 备注: 安装jdk1.8\n# 下载 $ git clone https://github.com/apache/spark.git # 解压 $ mkdir -p /usr/local/bin $ tar -zxvf jdk-8u241-linux-x64.tar.gz -C /usr/local/java $ export JAVA_HOME=/usr/local/bin/jdk1.8.0_241 $ export PATH=$JAVA_HOME/bin:$PATH # 执行编译 $ cd ./spark/ \u0026amp;\u0026amp; ./build/mvn -Pkubernetes -DskipTests clean package 提示：除了编译源码之外，还可以直接下载spark的release package。link：https://spark.apache.org/downloads.html\n构建镜像 缺省软件镜像源很慢, 建议修改./spark/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile并添加如下腾讯云源。\nRUN sed -i 's/deb.debian.org/mirrors.tencentyun.com/g' /etc/apt/sources.list \u0026amp;\u0026amp; \\ sed -i 's/security.debian.org/mirrors.tencentyun.com/g' /etc/apt/sources.list 开始构建基础镜像并上传到ccr仓库\n# 构建 sudo ./bin/docker-image-tool.sh -r ccr.ccs.tencentyun.com/hale -t 2.4.5 build # 上传 sudo ./bin/docker-image-tool.sh -r ccr.ccs.tencentyun.com/hale -t 2.4.5 push 应用代码准备 一般来说，应用代码逻辑会编译在同一份jar文件中，Driver和executor运行过程中都会用到这个jar，最新的spark已经支持以下两种方式提供：\n1 远程，把应用代码放到远程，比如hdfs或者http server，然后以远程uri的方式（http://）提供。一般推荐这种方式。\n比如临时启动http server, 然后通过http://10.0.0.172:8000/来访问。\n$ cd /root/spark-2.4.5-bin-hadoop2.7/examples/jars \u0026amp;\u0026amp; python -m SimpleHTTPServer 2 本地，把应用代码提前打包到自建的镜像中，然后在命令行中以local://的方式提供。这种方式需要每次提交代码后再重新制作镜像，相对远程方式来说比较麻烦。\n提示：直接从运行submit所在客户端的本地文件系统中提供应用代码，当前是不支持的。\n集群准备 1 打开集群的外网访问或者内网访问。\n2 创建命名空间（比如spark）并确保成功下发镜像仓库秘钥qcloudregistrykey。\n连接集群 1 client mode\n在client模式下，driver能够运行在集群外的vm上，但是driver必须要能够与集群的apiserver通信，TKE/EKS默认提供的是证书+token的认证方式（在控制台上通过集群管理\u0026ndash;\u0026gt;基本信息\u0026ndash;\u0026gt;集群凭证获取）。\n另外，TKE/EKS集群提供的是自签名证书，java/scala代码缺省不认，因此，需要先将自签名证书导入到vm的证书库中，成为可信任证书。\n# 添加 $ keytool -importcert -alias sparkonk8s -keystore cacerts -storepass changeit -file ./ca.crt # 删除 $ keytool -delete -v -keystore cacerts -storepass changeit --alias sparkonk8s 接下来，就可以在集群外的vm上执行spark-submit了，命令如下，其中应用代码以远程方式（http:/）提供：\n$ $SPARK_HOME/bin/spark-submit \\  --master k8s://https://$CLS_DOMIN:443 \\  --deploy-mode client \\  --class org.apache.spark.examples.SparkPi \\  --conf spark.kubernetes.container.image=$IMAGE_REPO/spark:2.4.5 \\  --conf spark.kubernetes.authenticate.caCertFile=/root/test/ca.crt \\  --conf spark.kubernetes.authenticate.oauthToken=HBlC0S64r8y2EUfqTxxxxFDzHlG5lub \\  --conf spark.executor.instances=2 \\  --conf spark.kubernetes.namespace=spark \\  --conf spark.kubernetes.container.image.pullSecrets=qcloudregistrykey \\  http://10.0.0.172:8000/examples/jars/spark-examples_2.11-2.4.5.jar 2 cluster mode\n在cluster模式下，driver和excutor都以pod的方式运行在集群中，同时，driver可以直接通过serviceaccount与集群的apisever认证，也比较方便。因此，一般推荐用户使用这种方式。\n创建serviceaccount和rolebinding的命令如下：\n$ kubectl create serviceaccount spark --namespace spark $ kubectl create rolebinding spark-edit --clusterrole=edit --serviceaccount=spark:spark --namespace=spark 接下来，根据spark-submit的执行环境，可以分为两种方式：\n2.1 集群外节点上以命令方式执行\n在集群外的vm节点上执行spark-submit命令的时候，spark-submit就需要与集群的apiserver建立通信了，可以参考client mode介绍的证书+token的认证方式与apiserver认证。命令如下，其中应用代码以本地方式（local:/）提供：\n$SPARK_HOME/bin/spark-submit \\  --master k8s://https://$CLS_DOMIN:443 \\  --deploy-mode cluster \\  --class org.apache.spark.examples.SparkPi \\  --conf spark.kubernetes.container.image=$IMAGE_REPO/spark:2.4.5 \\  --conf spark.kubernetes.authenticate.submission.caCertFile=/root/test/ca.crt \\  --conf spark.kubernetes.authenticate.submission.oauthToken=HBlC0S64r8y2EUfFDzHlG5lub \\  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\  --conf spark.executor.instances=2 \\  --conf spark.kubernetes.namespace=spark \\  --conf spark.kubernetes.container.image.pullSecrets=qcloudregistrykey \\ local:///opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar 2.2. 集群内以job的方式执行\nspark-submit运行在pod中，可以通过serviceAccountName: spark与apiserver通信。job的样例如下：\napiVersion: batch/v1 kind: Job metadata: name: spark-submit namespace: spark spec: template: metadata: name: spark-submit-example spec: serviceAccountName: spark containers: - name: spark-submit-example args: - /opt/spark/bin/spark-submit - --master - k8s://https://kubernetes.default.svc.cluster.local:443 - --deploy-mode - cluster - --conf - spark.kubernetes.container.image=ccr.ccs.tencentyun.com/hale/spark:2.4.5 - --conf - spark.kubernetes.authenticate.driver.serviceAccountName=spark - --conf - spark.kubernetes.namespace=spark - --conf - spark.executor.instances=2 - --conf - spark.kubernetes.container.image.pullSecrets=qcloudregistrykey - --class - org.apache.spark.examples.SparkPi - local:///opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar env: - name: SPARK_HOME value: /opt/spark resources: {} image: ccr.ccs.tencentyun.com/hale/spark:2.4.5 imagePullPolicy: Always 参数说明：\n  \u0026ndash;master： 指定要连接的k8s集群，格式为k8s://\u0026lt;api_server_host\u0026gt;:\u0026lt;k8s-apiserver-port\u0026gt;，其中api_server_host默认是https，而k8s-apiserver-port必须要显示指定，即使是443。\n  \u0026ndash;deploy-mode：部署模式，当前仅支持cluster和client这两种模式。\n  \u0026ndash;conf spark.kubernetes.authenticate.submission.caCertFile：submit任务时，与apiserver建立连接使用。当在client模式下，用spark.kubernetes.authenticate.caCertFile替代。\n  \u0026ndash;conf spark.kubernetes.authenticate.submission.oauthToken：submit任务时，与apiserver建立连接使用。当在client模式下，用spark.kubernetes.authenticate.oauthToken替代。\n  \u0026ndash;conf spark.kubernetes.authenticate.driver.serviceAccountName：指定driver pod使用的serviceAccountName。\n  \u0026ndash;conf spark.executor.instances：指定executors的具体实例数。\n  \u0026ndash;conf spark.kubernetes.namespace：指定driver和executors pod运行的命名空间。\n  应用代码提供的方式：当前支持远程（http:/）和本地（local:/），一般建议是使用远程方式，但是在cluster模式下，如果客户提前把应用代码放到了镜像中，也可以使用本地方式。\n  Driver 和 Executor 相关配置 1 镜像设置\n spark.kubernetes.container.image：指定容器镜像 spark.kubernetes.driver.container.image：指定driver pod的容器镜像，非必须 spark.kubernetes.executor.container.image：指定executor pod的容器镜像，非必须 spark.kubernetes.container.image.pullPolicy：指定镜像的拉取策略，默认为IfNotPresent spark.kubernetes.container.image.pullSecrets：自动镜像的拉取秘钥  2 资源设置\n不指定资源的情况下，driver和Executor的资源默认为\nresources: limits: memory: 1408Mi requests: cpu: \u0026#34;1\u0026#34; memory: 1408Mi   spark.driver.memory：设置driver的request.memory， limit.memory与request.memory保持一致\n  spark.executor.memory：设置executor的request.memory，limit.memory与request.memory保持一致\n  spark.driver.cores：设置driver的request.cpu， 当前进支持整数\n  spark.executor.cores：设置driver的request.cpu，当前进支持整数\n  spark.kubernetes.driver.limit.cores：设置driver的limit.cpu\n  spark.kubernetes.executor.request.cores：设置driver的request.cpu，可以覆盖spark.executor.cores设置的值\n  spark.kubernetes.executor.limit.cores：设置executor的limit.cpu\n  3 labels或者annotation设置\n spark.kubernetes.driver.label.[LabelName]：设置driver pod的label spark.kubernetes.driver.annotation.[AnnotationName]：设置driver pod的annotation spark.kubernetes.executor.label.[LabelName]：设置executor pod的label spark.kubernetes.executor.annotation.[AnnotationName]：设置executor pod的annotation  4 其他设置\n spark.kubernetes.node.selector.[labelKey]：设置driver 和executor pod的nodeSelector  检查相关 1 查看日志\n$ kubectl -n=\u0026lt;namespace\u0026gt; logs -f \u0026lt;driver-pod-name\u0026gt; 2 访问Driver UI\n当driver 运行在vm节点上时，可以直接访问http://\u0026lt;vm-ip\u0026gt;:4040\n当driver 运行在pod中时，可以通过如下方式访问：\n# 先执行kubeclt port-forward $ kubectl port-forward \u0026lt;driver-pod-name\u0026gt; 4040:4040 # 然后访问http://\u0026lt;node-ip\u0026gt;:4040 已知问题 1 eks要求实例规格配置\n运行在eks上的pod，都需要配置具体的实例规格，可以通过设置annotation的方式给配置实例规格，参考如下：\n# 给driver pod设置实例规格，以1c2g为例 spark.kubernetes.driver.annotation.eks.tke.cloud.tencent.com/cpu=1 spark.kubernetes.driver.annotation.eks.tke.cloud.tencent.com/mem=2Gi # 给executor pod设置实例规格，以1c2g为例 spark.kubernetes.executor.annotation.eks.tke.cloud.tencent.com/cpu=1 spark.kubernetes.executor.annotation.eks.tke.cloud.tencent.com/mem=2Gi 参考链接  https://spark.apache.org/docs/latest/running-on-kubernetes.html#running-spark-on-kubernetes https://github.com/kubernetes/kubernetes/issues/34377  "},{"uri":"https://book.kubetencent.io/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/bigdata-ai/tensorflow/","title":"TensorFlow on TKE/EKS","tags":[],"description":"","content":"简介 Kubeflow 开始是从支持tensorflow分布式训练演化而来，现在已经是一个由众多子项目组成的开源产品，愿景是在 Kubernetes 上运行各种机器学习工作负载， 包括tensorflow,pytorch等主流计算引擎，面向机器学习场景的工作流引擎argo等。支持从模型开发，模型训练到最终服务部署的全链条工具。采用tf-operator进行分布式tf训练可以方便提交任务，具有重试，容错等优势\n安装部署 安装 kubeflow curl -sL https://github.com/kubeflow/kfctl/releases/download/v1.0/kfctl_v1.0-0-g94c35cf_linux.tar.gz | tar -xzvf -C /home/ubuntu/kubeflow export PATH=$PATH:/home/ubuntu/kubeflow export KF_NAME=my-kubeflow export BASE_DIR=/home/ubuntu/kubeflow export KF_DIR=${BASE_DIR}/${KF_NAME} export CONFIG_URI=https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_k8s_istio.v1.0.0.yaml kfctl build -V -f $(CONFIG_URI) 安装 tf-operator kustomize build my-kubeflow/kustomize/tf-job-crds/overlays/application | kubectl create –f - kustomize build my-kubeflow/kustomize/tf-job-operator/overlays/application | kubectl create –f 测试 git clone https://github.com/tensorflow/k8s.git kubectl create –f k8s/examples/v1/dist-mnist/tf_job_mnist.yaml 使用kube-batch调度器 k8s缺省的调度器以pod为单位进行调度,也不提供按训练任务优先级优先级进行整体调度的功能，但是分布式机器学习会同时启动多种task，比如PS,WORKER，而且每种task都会有多个副本，这样有可能出现一个任务的部分pod被创建，而其他pod处于pending状态，出现资源被占用但是训练任务不能被启动执行的状态。kube-batch就是解决这种问题的批调度器，另外也提供队列和资源公平调度drf等功能。\n部署kube-batch git clone http://github.com/kubernetes-sigs/kube-batch cd kube-batch/deployment/kube-batch helm install --name kube-batch --namespace kube-system 缺省安装后kube-batch缺乏必要的权限来lw集群资源，需要额外创建RBAC，添加对应serviceaccount的cluster-admin权限用于lw集群中的node, pod等对象来维护自身的资源视图\n修改tf-operator 启动参数添加enable-gang-scheduling=true\ntf-operator缺省使用volcano调度器，添加启动参数gang-scheduler-name=kube-batch, （或者在TFJOB的申请中指定调度器名）\n修改RBAC添加tf-operator serviceaccount访问podgroups资源的权限\n测试 apiVersion: scheduling.incubator.k8s.io/v1alpha1 kind: PodGroup metadata: name: tfjob-group spec: minMember: 6 --- apiVersion: \u0026#34;kubeflow.org/v1\u0026#34; kind: \u0026#34;TFJob\u0026#34; metadata: name: \u0026#34;dist-mnist-for-e2e-test\u0026#34; spec: tfReplicaSpecs: PS: replicas: 2 restartPolicy: Never template: metadata: annotations: scheduling.k8s.io/group-name: tfjob-group spec: schedulerName: kube-batch containers: - name: tensorflow image: ccr.ccs.tencentyun.com/chenyong/tf-dist-mnist-test:1.0 Worker: replicas: 4 restartPolicy: Never template: metadata: annotations: scheduling.k8s.io/group-name: tfjob-group spec: schedulerName: kube-batch containers: - name: tensorflow image: ccr.ccs.tencentyun.com/chenyong/tf-dist-mnist-test:1.0 使用volcano调度器 部署 kubectl apply -f https://raw.githubusercontent.com/volcano-sh/volcano/master/installer/volcano-development.yaml 修改tf-operator启用volcano调度器 tf-operator缺省使用volcano，只要添加enable-gang-scheduling=true启动参数就可以，非常方便\n测试 apiVersion: scheduling.volcano.sh/v1beta1 kind: PodGroup metadata: name: dist-mnist-for-e2e-test spec: minMember: 6 --- apiVersion: \u0026#34;kubeflow.org/v1\u0026#34; kind: \u0026#34;TFJob\u0026#34; metadata: name: \u0026#34;dist-mnist-for-e2e-test\u0026#34; spec: tfReplicaSpecs: PS: replicas: 2 restartPolicy: Never template: spec: containers: - name: tensorflow image: ccr.ccs.tencentyun.com/chenyong/tf-dist-mnist-test:1.0 Worker: replicas: 4 restartPolicy: Never template: spec: containers: - name: tensorflow image: ccr.ccs.tencentyun.com/chenyong/tf-dist-mnist-test:1.0 直接使用volcano进行训练 volcano除了包含批调度器外，还实现了一组CRD和controller， 通过创建CRD就可以直接启动分布式tensorflow的训练任务\napiVersion: batch.volcano.sh/v1alpha1 kind: Job metadata: name: tensorflow-dist-mnist spec: minAvailable: 3 schedulerName: volcano plugins: env: [] svc: [] policies: - event: PodEvicted action: RestartJob tasks: - replicas: 1 name: ps template: spec: containers: - command: - sh - -c - | PS_HOST=`cat /etc/volcano/ps.host | sed \u0026#39;s/$/\u0026amp;:2222/g\u0026#39; | sed \u0026#39;s/^/\u0026#34;/;s/$/\u0026#34;/\u0026#39; | tr \u0026#34;\\n\u0026#34; \u0026#34;,\u0026#34;`; WORKER_HOST=`cat /etc/volcano/worker.host | sed \u0026#39;s/$/\u0026amp;:2222/g\u0026#39; | sed \u0026#39;s/^/\u0026#34;/;s/$/\u0026#34;/\u0026#39; | tr \u0026#34;\\n\u0026#34; \u0026#34;,\u0026#34;`; export TF_CONFIG={\\\u0026#34;cluster\\\u0026#34;:{\\\u0026#34;ps\\\u0026#34;:[${PS_HOST}],\\\u0026#34;worker\\\u0026#34;:[${WORKER_HOST}]},\\\u0026#34;task\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;ps\\\u0026#34;,\\\u0026#34;index\\\u0026#34;:${VK_TASK_INDEX}},\\\u0026#34;environment\\\u0026#34;:\\\u0026#34;cloud\\\u0026#34;}; python /var/tf_dist_mnist/dist_mnist.py image: volcanosh/dist-mnist-tf-example:0.0.1 name: tensorflow ports: - containerPort: 2222 name: tfjob-port resources: {} restartPolicy: Never - replicas: 2 name: worker policies: - event: TaskCompleted action: CompleteJob template: spec: containers: - command: - sh - -c - | PS_HOST=`cat /etc/volcano/ps.host | sed \u0026#39;s/$/\u0026amp;:2222/g\u0026#39; | sed \u0026#39;s/^/\u0026#34;/;s/$/\u0026#34;/\u0026#39; | tr \u0026#34;\\n\u0026#34; \u0026#34;,\u0026#34;`; WORKER_HOST=`cat /etc/volcano/worker.host | sed \u0026#39;s/$/\u0026amp;:2222/g\u0026#39; | sed \u0026#39;s/^/\u0026#34;/;s/$/\u0026#34;/\u0026#39; | tr \u0026#34;\\n\u0026#34; \u0026#34;,\u0026#34;`; export TF_CONFIG={\\\u0026#34;cluster\\\u0026#34;:{\\\u0026#34;ps\\\u0026#34;:[${PS_HOST}],\\\u0026#34;worker\\\u0026#34;:[${WORKER_HOST}]},\\\u0026#34;task\\\u0026#34;:{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;worker\\\u0026#34;,\\\u0026#34;index\\\u0026#34;:${VK_TASK_INDEX}},\\\u0026#34;environment\\\u0026#34;:\\\u0026#34;cloud\\\u0026#34;}; python /var/tf_dist_mnist/dist_mnist.py image: volcanosh/dist-mnist-tf-example:0.0.1 name: tensorflow ports: - containerPort: 2222 name: tfjob-port resources: {} restartPolicy: Never tf-operator on EKS tf-operator也可以在eks平台上部署和调度任务，但是目前还不支持扩展调度器，如果eks资源池足够大，不会出现部分pod被调度的情况\n"},{"uri":"https://book.kubetencent.io/faq/tke/","title":"TKE 常见问题","tags":[],"description":"","content":"为什么挂载的 CFS 容量是 10G 创建负载的时候可以挂载 NFS，如果使用 CFS 提供的 NFS 服务，当 NFS 被挂载好去查看实际容量时发现是 10G。\n原因是: nfs的盘，默认的挂载大小为10G，支持自动扩容\ncfs 扩容方式: 小于1T，容量到50%触发自动扩容，每次加100G; 大于 1T,到80%触发自动扩容。每次加500G\ncfs 收费方式: 计费是根据实际的使用量，小于10G不收费，多于10G开始收费\n为什么 controller-manager 和 scheduler 状态显示 Unhealthy kubectl get cs NAME STATUS MESSAGE ERROR scheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: getsockopt: connection refused controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: getsockopt: connection refused etcd-0 Healthy {\u0026#34;health\u0026#34;: \u0026#34;true\u0026#34;} 查看组件状态发现 controller-manager 和 scheduler 状态显示 Unhealthy，但是集群正常工作，是因为TKE metacluster托管方式集群的 apiserver 与 controller-manager 和 scheduler 不在同一个节点导致的，这个不影响功能。如果发现是 Healthy 说明 apiserver 跟它们部署在同一个节点，所以这个取决于部署方式。\n更详细的原因：\napiserver探测controller-manager 和 scheduler写死直接连的本机\nfunc (s componentStatusStorage) serversToValidate() map[string]*componentstatus.Server { serversToValidate := map[string]*componentstatus.Server{ \u0026#34;controller-manager\u0026#34;: {Addr: \u0026#34;127.0.0.1\u0026#34;, Port: ports.InsecureKubeControllerManagerPort, Path: \u0026#34;/healthz\u0026#34;}, \u0026#34;scheduler\u0026#34;: {Addr: \u0026#34;127.0.0.1\u0026#34;, Port: ports.InsecureSchedulerPort, Path: \u0026#34;/healthz\u0026#34;}, } 源码：https://github.com/kubernetes/kubernetes/blob/v1.14.3/pkg/registry/core/rest/storage_core.go#L256\n相关 issue:\n https://github.com/kubernetes/kubernetes/issues/19570 https://github.com/kubernetes/enhancements/issues/553  为什么 kubectl top nodes 不行 执行 kubectl top nodes 报 NotFound:\n$ kubectl top nodes Error from server (NotFound): the server could not find the requested resource 这是因为 metrics api 的 apiservice 指向的 TKE 自带的 hpa-metrics-server，而 hpa-metrics-server 不支持 node 相关的指标，hpa-metrics-server 主要用于 HPA 功能(Pod 横向自动伸缩)，伸缩判断指标最开始使用的 metrics api，后来改成了 custom metrics api。\nkubectl top nodes 是请求的 metrics api，由于 TKE 控制台的 HPA 功能不依赖 metrics api 了，可以自行修改其 apiservice 的指向为自建的 metrics server，比如最简单的官方开源的 metrics-server，参考 在 TKE 中安装 metrics-server，或者如果你集群中使用 prometheus，可以安装 k8s-prometheus-adapter 来适配 metrics api，也可以直接部署 kube-prometheus 安装更全面的 prometheus on kubernetes 全家桶套件。\n不管怎样，最后确保 v1beta1.metrics.k8s.io 这个 apiservice 指向了你自己的 metrics api 适配服务，通过 kubectl -n kube-system edit apiservice v1beta1.metrics.k8s.io 可以修改。\n我从 TKE 集群所在 VPC 之外有办法能直接访问容器 IP 吗 如果你的网络已经通过专线、对等连接、云联网或 VPN 等方式跟 TKE 集群所在 VPC 做了打通，可以参考 容器路由互通 来进一步打通容器路由。\nTKE中的容器和IDC自建k8s中的容器网络可以互通么？ 这个问题分两种情况，分别说明： （1） IDC k8s 中的容器访问TKE 中的容器。 完全可以，tke 的容器网络默认是基于全局路由的模式，IDC 和 腾讯云拉通专线（或者接入云联网）后，只要将TKE 集群的容器路由下发到IDC 的网关，就可以实现访问。\n（2） TKE 的容器访问IDC k8s 中的容器。 不一定：要看自建k8s 集群使用的网络模式，能否把容器路由发布到腾讯云的网关。 如果自建k8s 使用使用3层路由（如calico 的bgp模式），或者大二层（如macvlan）的模式网络模式，则可以将容器路由发布到云上，实现访问。 如果自建k8s 使用隧道网络模式（如flannel 或者calico 的ipip 模式），无法将路由发布出来，则无法访问。\nIDC 集群是否可以纳管云上的cvm 么？ 理论上可行，但不建议这么做，建议使用多集群方案，主要有以下几个原因：\n IDC和云上cvm 依赖专线或者云联网互通，网络抖动和延迟都会对集群管理带来影响。 IDC的网络环境和云上不一样，需要根据实际情况选择合适的网络插件，导致IDC 和 云上的网络插件不一致，使容器网络的管理复杂度增高： 目前还没有这样的客户案例，未经过生产验证，风险较高。 通过多集群来避免但集群故障，使用原生fedration 或者 一些开源工具（如rancher）来管理多集群也很方便。  在TKE Pod 中为什么无法访问到公网？ 在TKE 中，podip 首先snat 成nodeip, 然后通过nodeip 出公网。 podip snat 成nodeip 是通过k8s 组件 ip-masq-agent 修改iptables 规则来实现的。 TKE 默认 ip-masq-agent-config 配置在vpc 内不做snat, vpc 之外都做snat.\n所以如果pod 无法访问公网，需要分两种情况检查以下配置：\n 如果node可以访问公网； 检查ip-masq-agent-config 配置，podip 出公网时是否做了snat 。 如果node 不可以访问公网   检查/etc/resolv.conf ，看nameserver 配置是否正确，腾讯云主机默认nameserver如下： nameserver 183.60.83.19 nameserver 183.60.82.98 如果域名解析没有问题，那就应该是公网ip 或者nat网关的问题，联系相关同事排查。  cfs挂载在pod下的时候root权限，我的pod启动是普通用户，没法写入cfs，怎么解决，你们有什么方案吗? https://cloud.tencent.com/document/product/582/10951\n如cfs 文档所示，默认情况下文件系统的权限是755， 只有root 用户有写权限。 如果非root 用户要写，那么有两种情况：\n 非root 用户在root 组，要求文件系统的权限为 775 (rwxrwxr-x) 非root 用户不在root 组，要求文件系统权限为 777 (rwxrwxrwx) 总之用户要有对应的写权限。  具体如何改文件系统权限，可以咨询下cfs 的同事。 一种做法是：将nfs 挂载到本地，通过chmod 来改。\n"},{"uri":"https://book.kubetencent.io/containerization/","title":"业务容器化","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/security/","title":"安全管控","tags":[],"description":"","content":"  审计管理   应用权限管理   用户权限管理   "},{"uri":"https://book.kubetencent.io/security/audit/","title":"审计管理","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/security/app/","title":"应用权限管理","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/ingress/other/","title":"开源 Ingress 方案","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/security/user/","title":"用户权限管理","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/","title":"腾讯云容器服务指南","tags":[],"description":"","content":" 本书还正在起草，请关注文章头部的成熟度提示\n 在线阅读 地址: https://book.kubetencent.io\n目录  产品指引    TKE     EKS     TKE Mesh     TCR     更新历史      集群网络    彻底理解集群网络     TKE 集群网络介绍     网络划分与最大节点/service/pod 的数量     容器路由互通      Ingress    彻底理解 Ingress     Ingress 方案选型     TKE Ingress     开源 Ingress 方案    Nginx Ingress on TKE     Traefik Ingress on TKE       存储     附加组件    在 TKE 中安装 metrics-server      监控告警    在 TKE 上搭建 Prometheus 监控系统      日志搜集     CI/CD     大数据与 AI    GPU 虚拟化     Flink    Flink 介绍     Session Cluster 模式部署     Job Cluster 模式部署     Native Kubernetes 模式部署      Flink on TKE/EKS     Spark on TKE/EKS     TensorFlow on TKE/EKS      FAQ    EKS 常见问题     TKE 常见问题     镜像仓库常见问题      Serverless     Servie Mesh     业务容器化    获取源 IP     固定 IP     Spring Cloud     Dubbo      安全管控    审计管理     应用权限管理     用户权限管理      避坑指南     项目源码 项目源码存放于 Github 上: https://github.com/TencentCloudContainerTeam/book\nLicense 署名-非商业性使用-相同方式共享 4.0 (CC BY-NC-SA 4.0)\n"},{"uri":"https://book.kubetencent.io/damn/","title":"避坑指南","tags":[],"description":"","content":""},{"uri":"https://book.kubetencent.io/faq/registry/","title":"镜像仓库常见问题","tags":[],"description":"","content":"可以将自建的 harbor 镜像仓库同步到同步到 ccr 吗 当前 harbor 1.9 及以上可以直接在管理页面配置同步到 ccr，选择 docker registry 类型(ccr 兼容 registry 协议)。\n"}]